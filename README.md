# Generating Shakespearean-style Quotes with a Generatively Pretrained Transformer

This project was designed to achieve several objectives:

Develop a deeper understanding of the transformer architecture.
Implement the multi-head attention mechanism.
Explore the advantages of transformers compared to traditional NLP methods.
For this project, I used Andrej Karpathy’s demonstration of a GPT model as a starting point.

While the current model generates sentences at the word level, resulting in occasional incoherent phrases, there are several future enhancements I’d like to explore. The next steps include experimenting with more sophisticated tokenization approaches, such as SentencePiece, and training the model on different datasets to evaluate its performance across various language styles.

